{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87762e1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-01 16:40:56.554828: I tensorflow/core/platform/cpu_feature_guard.cc:183] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import math\n",
    "import numpy as np\n",
    "import math\n",
    "import numpy\n",
    "import random\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tensorflow.keras import models, Model\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "import matplotlib as mpl\n",
    "from matplotlib import cm\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from matplotlib.colors import ListedColormap\n",
    "import multiprocessing\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models, Model\n",
    "import numpy as np\n",
    "\n",
    "class DataSet():\n",
    "    def __init__(self, num_words=None, seed=113, maxlen=100, train_portion=0.5, long_first=False):\n",
    "        \"\"\"\n",
    "        :param num_words: Size of vocabulary, all words out-of-vocabulary will be marked as [unk]\n",
    "        :param seed: Seed for randomly shuffling dataset\n",
    "        :param maxlen: Maximum length of review, reviews longer than maxlen will be truncated, reviews shorter than\n",
    "        maxlen will be padded to maxlen\n",
    "        :param train_portion: Portion of dataset to allocate to training data, test data will be 1-train_portion\n",
    "        :param long_first: Sort reviews by length\n",
    "        \"\"\"\n",
    "        # retrieve IMDb data, x is a sequence containing movie review,\n",
    "        # y is a label indicating if it is positive or negative sentiment\n",
    "        (self.x_train, self.y_train), (self.x_test, self.y_test) = imdb.load_data(num_words=num_words, seed=seed)\n",
    "\n",
    "        if long_first:\n",
    "            self.y_train = [y for _, y in sorted(zip(self.x_train, self.y_train), key=lambda s: len(s[0]), reverse=True)]\n",
    "            self.x_train = sorted(self.x_train, key=len, reverse=True)\n",
    "\n",
    "            self.y_test = [y for _, y in sorted(zip(self.x_test, self.y_test), key=lambda s: len(s[0]), reverse=True)]\n",
    "            self.x_test = sorted(self.x_test, key=len, reverse=True)\n",
    "\n",
    "        self.total_length = [len(x) for x in self.x_train]\n",
    "        temp = [len(x) for x in self.x_test]\n",
    "        self.total_length.extend(temp)\n",
    "\n",
    "        # padding sequences to all be of the same length\n",
    "        self.x_train = pad_sequences(self.x_train, maxlen=maxlen, padding='post', truncating='post')\n",
    "        self.x_test = pad_sequences(self.x_test, maxlen=maxlen, padding='post', truncating='post')\n",
    "\n",
    "        self.split_data(train_portion)\n",
    "\n",
    "        self.word_index = imdb.get_word_index()\n",
    "        self.word_to_index = {word:id + 3 for word, id in self.word_index.items()}\n",
    "        self.word_to_index[\"[pad]\"] = 0\n",
    "        self.word_to_index[\"[start]\"] = 1\n",
    "        self.word_to_index[\"[unk]\"] = 2\n",
    "        self.word_to_index[\"[unused]\"] = 3\n",
    "        self.index_to_word = {i:word for (word, i) in self.word_to_index.items()}\n",
    "        self.form_vocab()\n",
    "\n",
    "    def form_vocab(self):\n",
    "        i2w_vocab = {}\n",
    "        w2i_vocab = {}\n",
    "\n",
    "        for sentence in self.x_train:\n",
    "            for id in sentence:\n",
    "                if id not in i2w_vocab:\n",
    "                    word = self.index_to_word[id]\n",
    "                    i2w_vocab[id] = word\n",
    "                    w2i_vocab[word] = id\n",
    "\n",
    "        for sentence in self.x_test:\n",
    "            for id in sentence:\n",
    "                if id not in i2w_vocab:\n",
    "                    word = self.index_to_word[id]\n",
    "                    i2w_vocab[id] = word\n",
    "                    w2i_vocab[word] = id\n",
    "        self.i2w_vocab = i2w_vocab\n",
    "        self.w2i_vocab = w2i_vocab\n",
    "\n",
    "    def get_data(self):\n",
    "        return (self.x_train, self.y_train), (self.x_test, self.y_test), (self.train_length, self.test_length)\n",
    "\n",
    "    def get_vocab_length(self):\n",
    "        return len(self.i2w_vocab)\n",
    "\n",
    "    # splits data into ratio train:test -> (train_portion:1-train_portion)\n",
    "    def split_data(self, train_portion):\n",
    "        x = np.concatenate((self.x_train, self.x_test), axis=0)\n",
    "        y = np.concatenate((self.y_train, self.y_test), axis=0)\n",
    "        self.train_length, self.test_length = self.total_length[:math.floor(train_portion * len(x))], \\\n",
    "                                              self.total_length[math.floor(train_portion * len(x)):]\n",
    "        self.x_train, self.x_test = x[:math.floor(train_portion * len(x))], \\\n",
    "                                    x[math.floor(train_portion * len(x)):]\n",
    "        self.y_train, self.y_test = y[:math.floor(train_portion * len(y))], \\\n",
    "                                    y[math.floor(train_portion * len(y)):]\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba1d02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy\n",
    "import random\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tensorflow.keras import models, Model\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "import matplotlib as mpl\n",
    "from matplotlib import cm\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from matplotlib.colors import ListedColormap\n",
    "import multiprocessing\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models, Model\n",
    "import numpy as np\n",
    "from numba import guvectorize,cuda,jit\n",
    "\n",
    "def get_mean_vector(h_set):\n",
    "    \"\"\"\n",
    "    :param h_set: list of vectors h, each vector is output of LSTM layer at a timestep\n",
    "    :return: mean vector hbar\n",
    "    \"\"\"\n",
    "    hbar = h_set[0]\n",
    "    for i in range(1, len(h_set)):\n",
    "        hbar += h_set[i]\n",
    "    hbar = hbar / len(h_set)\n",
    "    return hbar\n",
    "\n",
    "def get_magnitude(vector):\n",
    "    \"\"\"\n",
    "    :param vector: 1D numpy array\n",
    "    :return: magnitude of vector\n",
    "    \"\"\"\n",
    "    magnitude = 0\n",
    "    for element in vector:\n",
    "        magnitude += element ** 2\n",
    "    return math.sqrt(magnitude)\n",
    "\n",
    "def get_norm(vector):\n",
    "    \"\"\"\n",
    "    :param vector: vector to normalise\n",
    "    :return: norm of vector\n",
    "    \"\"\"\n",
    "    return vector / get_magnitude(vector)\n",
    "\n",
    "def project(vector, basis):\n",
    "    \"\"\"\n",
    "    :param vector: vector to project onto basis\n",
    "    :param basis: basis for poincare map\n",
    "    :return: vector projected onto basis (dot product)\n",
    "    \"\"\"\n",
    "    return vector.dot(basis)\n",
    "\n",
    "def get_iterh(lstm, start, steps, inputs):\n",
    "    \"\"\"\n",
    "    get poincare mapping (projections at h_t, projections at h_{t+1})\n",
    "    :param lstm: trained LSTM_layer\n",
    "    :param start: starting input\n",
    "    :param num_steps: number of iterations to perform, length of intermediate_inputs has to be num_steps - 1\n",
    "    :param intermediate_inputs: list of x_t to input at each timestep, zero vectors if None, each vector has to be length start\n",
    "    :return: poincare mapping\n",
    "    \"\"\"\n",
    "    if inputs is None:\n",
    "        inputs = [np.zeros(len(start), dtype=np.float64) for _ in range(num_steps - 1)]\n",
    "\n",
    "    # get h_t at each timestep\n",
    "    h_t = [lstm.step(start)[-1]]\n",
    "    for i in range(steps - 1):\n",
    "        curr_h = lstm.step(inputs[i])[-1]\n",
    "        h_t.append(curr_h)\n",
    " # remove last element so h_t and h_{t+1} aligns\n",
    "    return [h_t[-1]]\n",
    "\n",
    "def get_poincare_mapping(lstm, start, steps,num_steps, inputs,intermediate_inputs=None):\n",
    "    \"\"\"\n",
    "    get poincare mapping (projections at h_t, projections at h_{t+1})\n",
    "    :param lstm: trained LSTM_layer\n",
    "    :param start: starting input\n",
    "    :param num_steps: number of iterations to perform, length of intermediate_inputs has to be num_steps - 1\n",
    "    :param intermediate_inputs: list of x_t to input at each timestep, zero vectors if None, each vector has to be length start\n",
    "    :return: poincare mapping\n",
    "    \"\"\"\n",
    "    if intermediate_inputs is None:\n",
    "        intermediate_inputs = [np.zeros(len(start), dtype=np.float64) for _ in range(num_steps - 1)]\n",
    "\n",
    "    # get h_t at each timestep\n",
    "    h_t = get_iterh(lstm, start, steps, inputs)\n",
    "    h_t_1 = [] # h_{t+1}\n",
    "    #print(h_t)\n",
    "    for i in range(num_steps - 1):\n",
    "        curr_h = lstm.step(intermediate_inputs[i])[-1]\n",
    "        h_t.append(curr_h)\n",
    "        h_t_1.append(curr_h)\n",
    "\n",
    "    h_t.pop() # remove last element so h_t and h_{t+1} aligns\n",
    "    return h_t, h_t_1\n",
    "\n",
    "def get_poincare_mapping_gau(lstm, start, steps,num_steps, inputs,intermediate_inputs=None):\n",
    "    \"\"\"\n",
    "    get poincare mapping (projections at h_t, projections at h_{t+1})\n",
    "    :param lstm: trained LSTM_layer\n",
    "    :param start: starting input\n",
    "    :param num_steps: number of iterations to perform, length of intermediate_inputs has to be num_steps - 1\n",
    "    :param intermediate_inputs: list of x_t to input at each timestep, zero vectors if None, each vector has to be length start\n",
    "    :return: poincare mapping\n",
    "    \"\"\"\n",
    "    if intermediate_inputs is None:\n",
    "        intermediate_inputs = [np.zeros(len(start), dtype=np.float64) for _ in range(num_steps - 1)]\n",
    "\n",
    "    # get h_t at each timestep\n",
    "    h_t = get_iterh(lstm, start, steps, inputs)\n",
    "    h_t[0]=np.array(h_t[0])+random.gauss(0,1e-8)\n",
    "    #print(h_t)\n",
    "    h_t_1 = [] # h_{t+1}\n",
    "    for i in range(num_steps - 1):\n",
    "        curr_h = lstm.step(intermediate_inputs[i])[-1]\n",
    "        h_t.append(curr_h)\n",
    "        h_t_1.append(curr_h)\n",
    "\n",
    "    h_t.pop() # remove last element so h_t and h_{t+1} aligns\n",
    "    return h_t, h_t_1\n",
    "\n",
    "\n",
    "def main(n):       \n",
    "    # numbers setup\n",
    "    class LSTM_layer():\n",
    "        @staticmethod\n",
    "        def sigmoid(x):\n",
    "            return 1 / (1 + np.exp(-x))\n",
    "\n",
    "        @staticmethod\n",
    "        def tanh(x): # for consistency\n",
    "            return np.tanh(x)\n",
    "\n",
    "        def __init__(self, weights):\n",
    "            \"\"\"\n",
    "            :param weights: weights of LSTM layer\n",
    "            \"\"\"\n",
    "            # transposing matrices for dot product\n",
    "            self.W, self.U, self.b = np.transpose(weights[0]), np.transpose(weights[1]), np.transpose(weights[2])\n",
    "            self.num_units = int(self.U.shape[1])\n",
    "            self.split_weights()\n",
    "            # LSTM trained stateless, initial C and h are zero vectors\n",
    "            self.C = np.zeros((self.num_units), dtype=np.float64)\n",
    "            self.h = np.zeros((self.num_units), dtype=np.float64)\n",
    "\n",
    "        def split_weights(self):\n",
    "            # weights are stored as (neuron_num, (i, f, c, o))\n",
    "            self.W_i = np.ascontiguousarray(self.W[:self.num_units, :])\n",
    "            self.W_f = np.ascontiguousarray(self.W[self.num_units:self.num_units * 2, :])\n",
    "            self.W_c = np.ascontiguousarray(self.W[self.num_units * 2:self.num_units * 3, :])\n",
    "            self.W_o = np.ascontiguousarray(self.W[self.num_units * 3:, :])\n",
    "\n",
    "            self.U_i = np.ascontiguousarray(self.U[:self.num_units, :])\n",
    "            self.U_f = np.ascontiguousarray(self.U[self.num_units:self.num_units * 2, :])\n",
    "            self.U_c = np.ascontiguousarray(self.U[self.num_units * 2:self.num_units * 3, :])\n",
    "            self.U_o = np.ascontiguousarray(self.U[self.num_units * 3:, :])\n",
    "\n",
    "            self.b_i = np.ascontiguousarray(self.b[:self.num_units])\n",
    "            self.b_f = np.ascontiguousarray(self.b[self.num_units:self.num_units * 2])\n",
    "            self.b_c = np.ascontiguousarray(self.b[self.num_units * 2:self.num_units * 3])\n",
    "            self.b_o = np.ascontiguousarray(self.b[self.num_units * 3:])\n",
    "\n",
    "        def step(self, x_t):\n",
    "            \"\"\"\n",
    "            Performs a timestep (propagating new input through layer)\n",
    "            :return: array of activations [ft, it, cc, cc_update, c_out, ot, ht]\n",
    "            \"\"\"\n",
    "            activations = []\n",
    "            # forget step\n",
    "            ft = self.get_ft(x_t)\n",
    "            activations.append(ft)\n",
    "            self.forget(ft)\n",
    "\n",
    "            # \"remembering\" step\n",
    "            it = self.get_it(x_t)\n",
    "            activations.append(it)\n",
    "            cc = self.get_CC(x_t)\n",
    "            activations.append(cc)\n",
    "            cc_update = self.get_CC_update(it, cc)\n",
    "            activations.append(cc_update)\n",
    "            self.remember(cc_update)\n",
    "\n",
    "            # output step\n",
    "            c_out = self.get_C_output()\n",
    "            activations.append(c_out)\n",
    "            ot = self.get_ot(x_t)\n",
    "            activations.append(ot)\n",
    "            output = self.output(c_out, ot)\n",
    "            activations.append(output)\n",
    "\n",
    "            return activations\n",
    "\n",
    "        def reset(self):\n",
    "            # call when done with one input (with all timesteps completed)\n",
    "            # resets internal cell state and starting hidden state\n",
    "            self.C = np.zeros((self.num_units), dtype=np.float64)\n",
    "            self.h = np.zeros((self.num_units), dtype=np.float64)\n",
    "\n",
    "\n",
    "        # vectorized activation propagation\n",
    "        @staticmethod\n",
    "        @guvectorize(\n",
    "            [\"float64[:, :], float64[:, :], float64[:], float64[:], float64[:], float64[:]\"],\n",
    "            \"(n, m),(n, n),(m),(n),(n)->(n)\"\n",
    "  \n",
    "        )\n",
    "        def get_ft_vec(W_f, U_f, x_t, h, b_f, res):\n",
    "            wfx = W_f.dot(x_t)\n",
    "            ufh = U_f.dot(h)\n",
    "            sum_int = wfx + ufh\n",
    "            sum_f = sum_int + b_f\n",
    "            res[:] = 1 / (1 + np.exp(-sum_f))\n",
    "\n",
    "        @staticmethod\n",
    "        @guvectorize(\n",
    "            [\"float64[:, :], float64[:, :], float64[:], float64[:], float64[:], float64[:]\"],\n",
    "            \"(n, m),(n, n),(m),(n),(n)->(n)\"\n",
    "    \n",
    "        )\n",
    "        def get_it_vec(W_i, U_i, x_t, h, b_i, res):\n",
    "            wix = W_i.dot(x_t)\n",
    "            uih = U_i.dot(h)\n",
    "            sum_int = wix + uih\n",
    "            sum_f = sum_int + b_i\n",
    "            res[:] = 1 / (1 + np.exp(-sum_f))\n",
    "\n",
    "        @staticmethod\n",
    "        @guvectorize(\n",
    "            [\"float64[:, :], float64[:, :], float64[:], float64[:], float64[:], float64[:]\"],\n",
    "            \"(n, m),(n, n),(m),(n),(n)->(n)\"\n",
    "     \n",
    "        )\n",
    "        def get_CC_vec(W_c, U_c, x_t, h, b_c, res):\n",
    "            wcx = W_c.dot(x_t)\n",
    "            uch = U_c.dot(h)\n",
    "            sum_int = wcx + uch\n",
    "            sum_f = sum_int + b_c\n",
    "            res[:] = np.tanh(sum_f)\n",
    "\n",
    "        @staticmethod\n",
    "        @guvectorize(\n",
    "            [\"float64[:, :], float64[:, :], float64[:], float64[:], float64[:], float64[:]\"],\n",
    "            \"(n, m),(n, n),(m),(n),(n)->(n)\"\n",
    "   \n",
    "        )\n",
    "        def get_ot_vec(W_o, U_o, x_t, h, b_o, res):\n",
    "            wox = W_o.dot(x_t)\n",
    "            uoh = U_o.dot(h)\n",
    "            sum_int = wox + uoh\n",
    "            sum_f = sum_int + b_o\n",
    "            res[:] = 1 / (1 + np.exp(-sum_f))\n",
    "\n",
    "        # activations start\n",
    "        # tanh activations don't see an improvement from vectorization (probably because tanh is already vectorized)\n",
    "        def get_ft(self, x_t):\n",
    "            # sigmoid(W_f . x_t + U_f . h_(t-1) + b_f) . is dot product\n",
    "            # wfx = self.W_f.dot(x_t)\n",
    "            # ufh = self.U_f.dot(self.h)\n",
    "            # return LSTM_layer.sigmoid(wfx + ufh + self.b_f)\n",
    "            return LSTM_layer.get_ft_vec(self.W_f, self.U_f, x_t, self.h, self.b_f)\n",
    "\n",
    "        def get_it(self, x_t):\n",
    "            # sigmoid(W_i . x_t + U_i . h_(t-1) + b_i)\n",
    "            # wix = self.W_i.dot(x_t)\n",
    "            # uih = self.U_i.dot(self.h)\n",
    "            # return LSTM_layer.sigmoid(wix + uih + self.b_i)\n",
    "            return LSTM_layer.get_it_vec(self.W_i, self.U_i, x_t, self.h, self.b_i)\n",
    "\n",
    "        def get_CC(self, x_t):\n",
    "            # candidate cell state before proportion\n",
    "            # tanh(W_c . x_t + U_c . h_(t-1) + b_c)\n",
    "            wcx = self.W_c.dot(x_t)\n",
    "            uch = self.U_c.dot(self.h)\n",
    "            return LSTM_layer.tanh(wcx + uch + self.b_c)\n",
    "            # return LSTM_layer.get_CC_vec(self.W_c, self.U_c, x_t, self.h, self.b_c)\n",
    "\n",
    "        def get_ot(self, x_t):\n",
    "            # sigmoid(W_o . x_t + U_o . h_(t-1) + b_o)\n",
    "            # wox = self.W_o.dot(x_t)\n",
    "            # uoh = self.U_o.dot(self.h)\n",
    "            # return LSTM_layer.sigmoid(wox + uoh + self.b_o)\n",
    "            return LSTM_layer.get_ot_vec(self.W_o, self.U_o, x_t, self.h, self.b_o)\n",
    "\n",
    "        def get_C_output(self):\n",
    "            # cell state output before proportion\n",
    "            # tanh(C_t)\n",
    "            return LSTM_layer.tanh(self.C)\n",
    "\n",
    "        def get_CC_update(self, it, cc):\n",
    "            # candidate cell state after proportion, for updating cell state\n",
    "            # it * cc, * is Hadamard product\n",
    "            return it * cc\n",
    "        # activations end\n",
    "\n",
    "\n",
    "        # state updates start\n",
    "        def forget(self, ft):\n",
    "            # update old cell state in the forget step\n",
    "            self.C = self.C * ft\n",
    "\n",
    "        def remember(self, cc_update):\n",
    "            # update old cell state with new information\n",
    "            self.C = self.C + cc_update\n",
    "\n",
    "        def output(self, c_output, ot):\n",
    "            # proportionate the cell output vector for new output and hidden state\n",
    "            self.h = c_output * ot\n",
    "            return self.h\n",
    "        # state updates end\n",
    "\n",
    "    \n",
    "    # state updates end\n",
    "    trace_length = 25 # number of newest lines to draw\n",
    "    ppf = 1 #4 # datapoints per frame\n",
    "    num_timesteps = 500\n",
    "    len_sequence = 75000\n",
    "    start_point = 0 #99500\n",
    "    end_point = 75000\n",
    "    num_cells = 60\n",
    "    av=[]\n",
    "    with open('weight/dataset_4000_70.pkl', 'rb') as f:\n",
    "        dataset = pickle.load(f)\n",
    "  \n",
    "    #i=950\n",
    "\n",
    "        \n",
    "   # while i<1101:\n",
    "\n",
    "    filepath='paper(2 and3)/paper2/weights.'+str(n)+'.hdf5'\n",
    "    model= models.load_model(filepath,compile=False)\n",
    "    \n",
    "    \n",
    "\n",
    "    lm=[]\n",
    "    _, x, length = dataset.get_data()\n",
    "    _, length = length\n",
    "    x, y = x\n",
    "    a=[]\n",
    "    for i in range(len(x)):\n",
    "        if 0 not in x[i]:\n",
    "            a.append(i)\n",
    "    \n",
    "    for i in a[1:111:5]:\n",
    "        x_in=x[i].reshape()\n",
    "        #x_in=np.zeros((1,500),int).tolist()\n",
    "        # print(length[i])\n",
    "        # if length[i] < 350:\n",
    "        #     continue\n",
    "        #x_in_gau = x[i].reshape((1, num_timesteps))+ random.gauss(0,1e-15)\n",
    "        #x_in_gau = x_in + random.gauss(0,0.0001)\n",
    "        #print(x_in[0])\n",
    "        #print(x_in_gau)\n",
    "        lstm_in = x_train_embedded[i]\n",
    "        lstm_in=np.float64(lstm_in)\n",
    "\n",
    "        a=model.layers[0].get_weights()\n",
    "        for e in range(len(a)):\n",
    "                a[e]=np.float64(a[e])\n",
    "        lstm = LSTM_layer(a)\n",
    "        start = lstm_in[0]\n",
    "            #print(np.array(start).shape)\n",
    "            #print(start==start_gau)\n",
    "                #intermediate_steps = np.zeros((75000,32))\n",
    "        intermediate_steps = np.zeros((75000,32))\n",
    "            #print(intermediate_steps)\n",
    "        h_t_late, h_t_1_late = get_poincare_mapping(lstm, start,500,len_sequence, lstm_in[1:], intermediate_steps)\n",
    "        h_t_late.append(h_t_1_late[-1])\n",
    "        hbar_late = get_mean_vector(h_t_late)\n",
    "\n",
    "        h_t_late_gau, h_t_1_late_gau = get_poincare_mapping_gau(lstm, start,500,len_sequence,lstm_in[1:],intermediate_steps)\n",
    "                #print(h_t_late_gau[0])\n",
    "        h_t_late_gau.append(h_t_1_late_gau[-1])\n",
    "        late_set=[]\n",
    "\n",
    "                #print(len(h_t_late_gau))\n",
    "            #print(len(h_t_late))\n",
    "        for j in range(len(h_t_late)-10, len(h_t_late)):\n",
    "\n",
    "                    vec1=np.array(h_t_late[j])\n",
    "\n",
    "                    vec2=np.array(h_t_late_gau[j])\n",
    "                    dist = numpy.linalg.norm(vec1 - vec2)\n",
    "\n",
    "                    dist_late=numpy.log(dist+numpy.exp(-25))\n",
    "\n",
    "                    late_set.append(dist_late)\n",
    "\n",
    "        lm.append(np.mean(late_set))\n",
    "        print(np.mean(late_set))\n",
    "    return np.mean(lm),max(lm)\n",
    "               \n",
    "\n",
    "\n",
    "        \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    #pool=multiprocessing.Pool(processes=8)\n",
    "    #for i in range(233,420):\n",
    "        #pool.apply_async(main, (i, ))\n",
    "        x=[]\n",
    "        for i in range(1,401):\n",
    "            x.append(main(i))\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
