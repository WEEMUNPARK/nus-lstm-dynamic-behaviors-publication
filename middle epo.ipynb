{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc5dab23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_train_step(model, inputs, targets, l2_strength):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(inputs, training=True)\n",
    "        loss = tf.keras.losses.binary_crossentropy(targets, predictions)\n",
    "\n",
    "        # Calculate L2 regularization term for LSTM kernel and recurrent weights\n",
    "        if l2_strength > 0.0:\n",
    "            lstm_weights = model.layers[1].trainable_weights  # Access the LSTM layer's weights\n",
    "            l2_reg = tf.add_n([tf.nn.l2_loss(w) for i,w in enumerate(lstm_weights) if i==1])\n",
    "            loss += l2_strength * l2_reg\n",
    "\n",
    "    # Compute gradients\n",
    "    trainable_weights = model.trainable_variables\n",
    "    gradients = tape.gradient(loss, trainable_weights)\n",
    "\n",
    "\n",
    "\n",
    "    # non update kernel and bias\n",
    "    for i, weight in enumerate(trainable_weights):\n",
    "        if i==2:\n",
    "            gradients[i] = tf.zeros_like(weight)\n",
    "\n",
    "\n",
    "    opt.apply_gradients(zip(gradients, trainable_weights))\n",
    "    return loss\n",
    "\n",
    "def custom_train_loop(model, x_train, y_train, x_test, y_test, epochs, batch_size,l2_strength):\n",
    "    num_batches = len(x_train) // batch_size\n",
    "    \n",
    "    #while epoch in range(100,200), since optimal mostly happen around this regime, we print middle epoch just on this conditon.\n",
    "    for epoch in range(0,100):\n",
    "        epoch_loss = 0.0\n",
    "        for batch in range(num_batches):\n",
    "            batch_inputs = x_train[batch * batch_size : (batch + 1) * batch_size]\n",
    "            batch_targets = y_train[batch * batch_size : (batch + 1) * batch_size]\n",
    "\n",
    "            # Reshape batch_targets to (batch_size, 1) to match the model output shape\n",
    "            batch_targets = batch_targets.reshape(-1, 1)\n",
    "\n",
    "            loss= custom_train_step(model, batch_inputs, batch_targets,l2_strength)\n",
    "            epoch_loss += tf.reduce_mean(loss)\n",
    "\n",
    "        avg_loss = epoch_loss / num_batches\n",
    "        print(\"Epoch {}: Loss: {:.4f}\".format(epoch + 1, avg_loss.numpy()))\n",
    "        y_test_reshaped = y_test.reshape(-1, 1)\n",
    "        val_loss,val_acc = model.evaluate(x_test, y_test_reshaped, verbose=0)\n",
    "\n",
    "        print(\"Validation Accuracy: {:.2f}%\".format(val_acc * 100))\n",
    "        print(\"Validation Loss: {:.4f}\".format(val_loss))\n",
    "        \n",
    "        path='unitestdense'+'/'+'weight.'+str(epoch+1)+'.txt'\n",
    "        np.savetxt(path,model.layers[1].get_weights()[0])\n",
    "        path1='unitestdense'+'/'+'weight_re.'+str(epoch+1)+'.txt'\n",
    "        np.savetxt(path1,model.layers[1].get_weights()[1])\n",
    "        path2='unitestdense'+'/'+'loss.'+str(epoch+1)+'.txt'\n",
    "        np.savetxt(path2,[avg_loss.numpy(),val_loss,val_acc])\n",
    "        path3='unitestdense'+'/'+'deweights.'+str(epoch+1)+'.txt'\n",
    "        np.savetxt(path3,model.layers[1].get_weights()[2])\n",
    "        path4='unitestdense'+'/'+'denseweights.'+str(epoch+1)+'.txt'\n",
    "        np.savetxt(path4,model.layers[2].get_weights()[0])\n",
    "        path5='unitestdense'+'/'+'densebweights.'+str(epoch+1)+'.txt'\n",
    "        np.savetxt(path5,model.layers[2].get_weights()[1])\n",
    "        \n",
    "        \n",
    "    #middle epochs, here now the batch_size is 128, so the mun_batches is 273, we devide it into 21 parts[13,26......273]    \n",
    "    for epoch in range(100,201):\n",
    "        div_batch=[13*i for i in range(1,20)]\n",
    "        div_batch.append(num_batches)\n",
    "        for i in range(len(div_batch)):\n",
    "            for batch in range(div_batch[i]):\n",
    "                epoch_loss = 0.0\n",
    "                batch_inputs = x_train[batch * batch_size : (batch + 1) * batch_size]\n",
    "                batch_targets = y_train[batch * batch_size : (batch + 1) * batch_size]\n",
    "\n",
    "                # Reshape batch_targets to (batch_size, 1) to match the model output shape\n",
    "                batch_targets = batch_targets.reshape(-1, 1)\n",
    "\n",
    "                loss= custom_train_step(model, batch_inputs, batch_targets,l2_strength)\n",
    "                epoch_loss += tf.reduce_mean(loss)\n",
    "\n",
    "            avg_loss = epoch_loss / div_batch[i]\n",
    "            print(\"Epoch {}: Loss: {:.4f}\".format(epoch + div_batch[i]/num_batches, avg_loss.numpy()))\n",
    "\n",
    "\n",
    "            # Reshape y_test to (batch_size, 1) to match the model output shape\n",
    "            y_test_reshaped = y_test.reshape(-1, 1)\n",
    "            val_loss,val_acc = model.evaluate(x_test, y_test_reshaped, verbose=0)\n",
    "\n",
    "            print(\"Validation Accuracy: {:.2f}%\".format(val_acc * 100))\n",
    "            print(\"Validation Loss: {:.4f}\".format(val_loss))\n",
    "            \n",
    "            path='unitestdense'+'/'+'weight.'+str(epoch+div_batch[i]/num_batches)+'.txt'\n",
    "            np.savetxt(path,model.layers[1].get_weights()[0])\n",
    "            path1='unitestdense'+'/'+'weight_re.'+str(epoch+div_batch[i]/num_batches)+'.txt'\n",
    "            np.savetxt(path1,model.layers[1].get_weights()[1])\n",
    "            path2='unitestdense'+'/'+'loss.'+str(epoch+div_batch[i]/num_batches)+'.txt'\n",
    "            np.savetxt(path2,[avg_loss.numpy(),val_loss,val_acc])\n",
    "            path3='unitestdense'+'/'+'deweights.'+str(epoch+div_batch[i]/num_batches)+'.txt'\n",
    "            np.savetxt(path3,model.layers[1].get_weights()[2])\n",
    "            path4='unitestdense'+'/'+'denseweights.'+str(epoch+div_batch[i]/num_batches)+'.txt'\n",
    "            np.savetxt(path4,model.layers[2].get_weights()[0])\n",
    "            path5='unitestdense'+'/'+'densebweights.'+str(epoch+div_batch[i]/num_batches)+'.txt'\n",
    "            np.savetxt(path5,model.layers[2].get_weights()[1])\n",
    "        \n",
    "        \n",
    "        \n",
    "    for epoch in range(201,epochs):\n",
    "        epoch_loss = 0.0\n",
    "        for batch in range(num_batches):\n",
    "            batch_inputs = x_train[batch * batch_size : (batch + 1) * batch_size]\n",
    "            batch_targets = y_train[batch * batch_size : (batch + 1) * batch_size]\n",
    "\n",
    "            # Reshape batch_targets to (batch_size, 1) to match the model output shape\n",
    "            batch_targets = batch_targets.reshape(-1, 1)\n",
    "\n",
    "            loss= custom_train_step(model, batch_inputs, batch_targets,l2_strength)\n",
    "            epoch_loss += tf.reduce_mean(loss)\n",
    "\n",
    "        avg_loss = epoch_loss / num_batches\n",
    "        print(\"Epoch {}: Loss: {:.4f}\".format(epoch + 1, avg_loss.numpy()))\n",
    "        y_test_reshaped = y_test.reshape(-1, 1)\n",
    "        val_loss,val_acc = model.evaluate(x_test, y_test_reshaped, verbose=0)\n",
    "\n",
    "        print(\"Validation Accuracy: {:.2f}%\".format(val_acc * 100))\n",
    "        print(\"Validation Loss: {:.4f}\".format(val_loss))\n",
    "        \n",
    "        path='unitestdense'+'/'+'weight.'+str(epoch+1)+'.txt'\n",
    "        np.savetxt(path,model.layers[1].get_weights()[0])\n",
    "        path1='unitestdense'+'/'+'weight_re.'+str(epoch+1)+'.txt'\n",
    "        np.savetxt(path1,model.layers[1].get_weights()[1])\n",
    "        path2='unitestdense'+'/'+'loss.'+str(epoch+1)+'.txt'\n",
    "        np.savetxt(path2,[avg_loss.numpy(),val_loss,val_acc])\n",
    "        path3='unitestdense'+'/'+'deweights.'+str(epoch+1)+'.txt'\n",
    "        np.savetxt(path3,model.layers[1].get_weights()[2])\n",
    "        path4='unitestdense'+'/'+'denseweights.'+str(epoch+1)+'.txt'\n",
    "        np.savetxt(path4,model.layers[2].get_weights()[0])\n",
    "        path5='unitestdense'+'/'+'densebweights.'+str(epoch+1)+'.txt'\n",
    "        np.savetxt(path5,model.layers[2].get_weights()[1])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bfacc4a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.047619047619047616"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "13/273"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62cb377",
   "metadata": {},
   "outputs": [],
   "source": [
    "iiiiii=[]\n",
    "for epoch in range(0,100) or in range(201,epochs):\n",
    "    iiiiii.append(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157e7596",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
