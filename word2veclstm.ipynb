{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f683d7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-12 14:35:33.661333: I tensorflow/core/platform/cpu_feature_guard.cc:183] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "class DataSet():\n",
    "    def __init__(self, num_words=None, seed=113, maxlen=100, train_portion=0.5, long_first=False):\n",
    "        \"\"\"\n",
    "        :param num_words: Size of vocabulary, all words out-of-vocabulary will be marked as [unk]\n",
    "        :param seed: Seed for randomly shuffling dataset\n",
    "        :param maxlen: Maximum length of review, reviews longer than maxlen will be truncated, reviews shorter than\n",
    "        maxlen will be padded to maxlen\n",
    "        :param train_portion: Portion of dataset to allocate to training data, test data will be 1-train_portion\n",
    "        :param long_first: Sort reviews by length\n",
    "        \"\"\"\n",
    "        # retrieve IMDb data, x is a sequence containing movie review,\n",
    "        # y is a label indicating if it is positive or negative sentiment\n",
    "        (self.x_train, self.y_train), (self.x_test, self.y_test) = imdb.load_data(num_words=num_words, seed=seed)\n",
    "\n",
    "        if long_first:\n",
    "            self.y_train = [y for _, y in sorted(zip(self.x_train, self.y_train), key=lambda s: len(s[0]), reverse=True)]\n",
    "            self.x_train = sorted(self.x_train, key=len, reverse=True)\n",
    "\n",
    "            self.y_test = [y for _, y in sorted(zip(self.x_test, self.y_test), key=lambda s: len(s[0]), reverse=True)]\n",
    "            self.x_test = sorted(self.x_test, key=len, reverse=True)\n",
    "\n",
    "        self.total_length = [len(x) for x in self.x_train]\n",
    "        temp = [len(x) for x in self.x_test]\n",
    "        self.total_length.extend(temp)\n",
    "\n",
    "        # padding sequences to all be of the same length\n",
    "        self.x_train = pad_sequences(self.x_train, maxlen=maxlen, padding='post', truncating='post')\n",
    "        self.x_test = pad_sequences(self.x_test, maxlen=maxlen, padding='post', truncating='post')\n",
    "\n",
    "        self.split_data(train_portion)\n",
    "\n",
    "        self.word_index = imdb.get_word_index()\n",
    "        self.word_to_index = {word:id + 3 for word, id in self.word_index.items()}\n",
    "        self.word_to_index[\"[pad]\"] = 0\n",
    "        self.word_to_index[\"[start]\"] = 1\n",
    "        self.word_to_index[\"[unk]\"] = 2\n",
    "        self.word_to_index[\"[unused]\"] = 3\n",
    "        self.index_to_word = {i:word for (word, i) in self.word_to_index.items()}\n",
    "        self.form_vocab()\n",
    "\n",
    "    def form_vocab(self):\n",
    "        i2w_vocab = {}\n",
    "        w2i_vocab = {}\n",
    "\n",
    "        for sentence in self.x_train:\n",
    "            for id in sentence:\n",
    "                if id not in i2w_vocab:\n",
    "                    word = self.index_to_word[id]\n",
    "                    i2w_vocab[id] = word\n",
    "                    w2i_vocab[word] = id\n",
    "\n",
    "        for sentence in self.x_test:\n",
    "            for id in sentence:\n",
    "                if id not in i2w_vocab:\n",
    "                    word = self.index_to_word[id]\n",
    "                    i2w_vocab[id] = word\n",
    "                    w2i_vocab[word] = id\n",
    "        self.i2w_vocab = i2w_vocab\n",
    "        self.w2i_vocab = w2i_vocab\n",
    "\n",
    "    def get_data(self):\n",
    "        return (self.x_train, self.y_train), (self.x_test, self.y_test), (self.train_length, self.test_length)\n",
    "\n",
    "    def get_vocab_length(self):\n",
    "        return len(self.i2w_vocab)\n",
    "\n",
    "    # splits data into ratio train:test -> (train_portion:1-train_portion)\n",
    "    def split_data(self, train_portion):\n",
    "        x = np.concatenate((self.x_train, self.x_test), axis=0)\n",
    "        y = np.concatenate((self.y_train, self.y_test), axis=0)\n",
    "        self.train_length, self.test_length = self.total_length[:math.floor(train_portion * len(x))], \\\n",
    "                                              self.total_length[math.floor(train_portion * len(x)):]\n",
    "        self.x_train, self.x_test = x[:math.floor(train_portion * len(x))], \\\n",
    "                                    x[math.floor(train_portion * len(x)):]\n",
    "        self.y_train, self.y_test = y[:math.floor(train_portion * len(y))], \\\n",
    "                                    y[math.floor(train_portion * len(y)):]\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "63b359ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow.keras.regularizers\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.compat.v1.keras.layers import CuDNNLSTM\n",
    "import numpy as np\n",
    "import matplotlib as matplot\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "import math\n",
    "# from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger, EarlyStopping\n",
    "import pickle\n",
    "from tensorflow.keras.layers import GaussianNoise\n",
    "maxlen = 500\n",
    "batch_size = 50\n",
    "VOCAB_size = 4000\n",
    "INPUT_SIZE=500\n",
    "dataset = DataSet(VOCAB_size, maxlen=INPUT_SIZE, train_portion=0.7)\n",
    "with open('unitc/dataset_4000_70.pkl', 'wb') as f:\n",
    "    pickle.dump(dataset, f)\n",
    "(x_train, y_train), (x_test, y_test),(_,_) = dataset.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "593027d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2Vec(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(Word2Vec, self).__init__()\n",
    "        self.target_embedding = layers.Embedding(vocab_size,\n",
    "                                          embedding_dim,\n",
    "                                          input_length=1,\n",
    "                                          name=\"w2v_embedding\")\n",
    "        self.context_embedding = layers.Embedding(vocab_size,\n",
    "                                           embedding_dim,\n",
    "                                           input_length=num_ns+1)\n",
    "\n",
    "    def call(self, pair):\n",
    "        target, context = pair\n",
    "            # target: (batch, dummy?)  # The dummy axis doesn't exist in TF2.7+\n",
    "            # context: (batch, context)\n",
    "        if len(target.shape) == 2:\n",
    "            target = tf.squeeze(target, axis=1)\n",
    "            # target: (batch,)\n",
    "        word_emb = self.target_embedding(target)\n",
    "            # word_emb: (batch, embed)\n",
    "        context_emb = self.context_embedding(context)\n",
    "            # context_emb: (batch, context, embed)\n",
    "        dots = tf.einsum('be,bce->bc', word_emb, context_emb)\n",
    "            # dots: (batch, context)\n",
    "        return dots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08d49e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(x_logit, y_true):\n",
    "      return tf.nn.sigmoid_cross_entropy_with_logits(logits=x_logit, labels=y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1047796e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "x_train=x_train.tolist()+x_test.tolist()\n",
    "x_train=np.array(x_train)\n",
    "# Create sentences for Word2Vec\n",
    "train_sentences = [[str(word) for word in sequence] for sequence in x_train]\n",
    "\n",
    "# Train Word2Vec model\n",
    "EMBEDDING_SIZE = 32\n",
    "CONTEXT_WINDOW = 5\n",
    "MIN_WORD_FREQUENCY = 5\n",
    "\n",
    "# Train Word2Vec model\n",
    "word2vec_model = Word2Vec(sentences=train_sentences, vector_size=EMBEDDING_SIZE, window=CONTEXT_WINDOW, min_count=MIN_WORD_FREQUENCY, sg=1)\n",
    "\n",
    "# Transform x_train sequences to (500*32) dimension vectors\n",
    "x_train_embedded = np.zeros((x_train.shape[0], maxlen, EMBEDDING_SIZE))\n",
    "for i, sequence in enumerate(train_sentences):\n",
    "    embedding_sequence = []\n",
    "    for word in sequence:\n",
    "        if word in word2vec_model.wv:\n",
    "            embedding_sequence.append(word2vec_model.wv[word])\n",
    "    x_train_embedded[i] = np.array(embedding_sequence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bc028bfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 500, 32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_embedded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5cae5a3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35000, 500)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test),(_,_) = dataset.get_data()\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3ac937a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('word2vec/x_train_embedded.pkl', 'wb') as f:\n",
    "    pickle.dump(x_train_embedded, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9956ab09",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('x_train_embedded.pkl', 'rb') as f:\n",
    "    x_train_embedded = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "35b25333",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 500, 32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689b9be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow.keras.regularizers\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.compat.v1.keras.layers import CuDNNLSTM\n",
    "import numpy as np\n",
    "import matplotlib as matplot\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "import math\n",
    "# from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger, EarlyStopping\n",
    "import pickle\n",
    "from tensorflow.keras.layers import GaussianNoise\n",
    "np.random.seed(42)\n",
    "#set up hyperparameters\n",
    "# max_features = 50000\n",
    "maxlen = 500\n",
    "batch_size = 50\n",
    "VOCAB_size = 4000\n",
    "INPUT_SIZE=500\n",
    "dataset = DataSet(VOCAB_size, maxlen=INPUT_SIZE, train_portion=0.7)\n",
    "with open('word2vec/dataset_4000_70.pkl', 'wb') as f:\n",
    "    pickle.dump(dataset, f)\n",
    "(_,_), (x_test, y_test),(_,_) = dataset.get_data()\n",
    "#vocab_size = dataset.get_vocab_length()\n",
    "with open('word2vec/x_train_embedded.pkl', 'rb') as f:\n",
    "    x_train_embedded = pickle.load(f)\n",
    "x_train=x_train_embedded[:35001]\n",
    "x_test=x_train_embedded[35001:]\n",
    "\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "        \n",
    "model_s = keras.Sequential()\n",
    "model_s.add(LSTM(32))\n",
    "model_s.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.0005)\n",
    "model_s.compile(loss='binary_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#def schedule(epoch):\n",
    "    #if epoch < 20:\n",
    "        #return 0.0005\n",
    "    #if epoch < 200:\n",
    "        #return 0.0001\n",
    "    #if epoch < 300:\n",
    "        #return 0.00005\n",
    "    #if epoch < 900:\n",
    "        #return 0.00001\n",
    "    #else:\n",
    "        #return 0.000005\n",
    "\n",
    "#lr_scheduler = tf.keras.callbacks.LearningRateScheduler(schedule)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#model.predict(x_test)\n",
    "\n",
    "\n",
    "\n",
    "checkpoint_filepath='updown/weights.{epoch:02d}.hdf5'\n",
    "\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    monitor= 'val_loss',\n",
    "    verbose= 0,\n",
    "    save_best_only= False,\n",
    "    save_weights_only = False,\n",
    "    mode= 'auto',\n",
    "    save_freq='epoch',\n",
    "    options=None,\n",
    "    initial_value_threshold=None\n",
    ")\n",
    "\n",
    "history = model_s.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=1000,\n",
    "                    validation_data=(x_test, y_test),\n",
    "                     callbacks=[model_checkpoint_callback])\n",
    "\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(len(acc))\n",
    "\n",
    "\n",
    "\n",
    "plt.plot(epochs, acc, 'r', label='Training accuracy')\n",
    "plt.plot(epochs, val_acc, 'b', label='validation accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'r', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='validation loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
