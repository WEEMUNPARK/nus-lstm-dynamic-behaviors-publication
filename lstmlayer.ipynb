{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1969edef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (35000, 500)\n",
      "x_test shape: (15000, 500)\n",
      "Epoch 1: Loss: 0.6727\n",
      "Validation Loss: 0.7107\n",
      "Epoch 2: Loss: 0.6617\n",
      "Validation Loss: 0.7149\n",
      "Epoch 3: Loss: 0.6601\n",
      "Validation Loss: 0.7086\n",
      "Epoch 4: Loss: 0.6537\n",
      "Validation Loss: 0.7182\n",
      "Epoch 5: Loss: 0.6440\n",
      "Validation Loss: 0.8531\n",
      "Epoch 6: Loss: 0.5692\n",
      "Validation Loss: 0.7059\n",
      "Epoch 7: Loss: 0.6546\n",
      "Validation Loss: 0.5872\n",
      "Epoch 8: Loss: 0.5007\n",
      "Validation Loss: 0.5178\n",
      "Epoch 9: Loss: 0.4797\n",
      "Validation Loss: 0.5197\n",
      "Epoch 10: Loss: 0.4695\n",
      "Validation Loss: 0.5415\n",
      "Epoch 11: Loss: 0.4679\n",
      "Validation Loss: 0.5365\n",
      "Epoch 12: Loss: 0.4657\n",
      "Validation Loss: 0.5407\n",
      "Epoch 13: Loss: 0.4732\n",
      "Validation Loss: 0.5352\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Layer, Embedding, Dense\n",
    "from tensorflow.compat.v1.keras.layers import CuDNNLSTM\n",
    "\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow.keras.regularizers\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.compat.v1.keras.layers import CuDNNLSTM\n",
    "import numpy as np\n",
    "import matplotlib as matplot\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "import math\n",
    "# from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger, EarlyStopping\n",
    "import pickle\n",
    "from tensorflow.keras.layers import GaussianNoise\n",
    "# set up hyperparameters\n",
    "# max_features = 50000\n",
    "maxlen = 500\n",
    "batch_size = 128\n",
    "VOCAB_size = 4000\n",
    "INPUT_SIZE=500\n",
    "\n",
    "dataset = DataSet(VOCAB_size, maxlen=INPUT_SIZE, train_portion=0.7)\n",
    "with open('innine/dataset_4000_70.pkl', 'wb') as f:\n",
    "    pickle.dump(dataset, f)\n",
    "(x_train, y_train), (x_test, y_test),(_,_) = dataset.get_data()\n",
    "\n",
    "\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "# 自定义CuDNNLSTM层，设置trainable=True\n",
    "class CustomCuDNNLSTM(CuDNNLSTM):\n",
    "    def __init__(self, units, **kwargs):\n",
    "        super(CustomCuDNNLSTM, self).__init__(units, **kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(CustomCuDNNLSTM, self).build(input_shape)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(CustomCuDNNLSTM, self).get_config()\n",
    "        return config\n",
    "\n",
    "# 创建一个Sequential模型\n",
    "model_s = tf.keras.Sequential()\n",
    "\n",
    "# 添加一个Embedding层，设置trainable=False，冻结weights\n",
    "model_s.add(Embedding(VOCAB_size, output_dim=32, input_length=500, weights=weights, trainable=False))\n",
    "\n",
    "# 添加一个自定义的CuDNNLSTM层\n",
    "model_s.add(CustomCuDNNLSTM(60,kernel_initializer=tf.keras.initializers.Constant(0.000001),recurrent_initializer=tf.keras.initializers.Constant(0.000001),bias_initializer='zeros',unit_forget_bias=False))\n",
    "\n",
    "# 添加一个Dense层，设置trainable=False，冻结weights2\n",
    "model_s.add(Dense(1, activation='sigmoid', weights=weights2, trainable=False))\n",
    "\n",
    "# 编译模型\n",
    "opt = tf.keras.optimizers.Adam()\n",
    "model_s.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "# 自定义训练循环\n",
    "def custom_train_step(model, inputs, targets):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(inputs, training=True)\n",
    "        loss = tf.keras.losses.binary_crossentropy(targets, predictions)\n",
    "\n",
    "    # 获取recurrent weights\n",
    "    trainable_weights = model.trainable_variables\n",
    "\n",
    "    # 获取所有可训练参数\n",
    "\n",
    "    # 计算梯度\n",
    "    gradients = tape.gradient(loss, trainable_weights)\n",
    "\n",
    "    # 将kernel weights和biases的梯度设为零，防止它们被更新\n",
    "    for i, weight in enumerate(trainable_weights):\n",
    "        if weight is model.layers[1].bias:\n",
    "            gradients[i] = tf.zeros_like(weight)\n",
    "\n",
    "    # 应用梯度更新\n",
    "    opt.apply_gradients(zip(gradients, trainable_weights))\n",
    "\n",
    "    return loss\n",
    "\n",
    "def custom_train_loop(model, x_train, y_train, x_test, y_test, epochs, batch_size):\n",
    "    num_batches = len(x_train) // batch_size\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0.0\n",
    "        for batch in range(num_batches):\n",
    "            batch_inputs = x_train[batch * batch_size : (batch + 1) * batch_size]\n",
    "            batch_targets = y_train[batch * batch_size : (batch + 1) * batch_size]\n",
    "\n",
    "            # Reshape batch_targets to (batch_size, 1) to match the model output shape\n",
    "            batch_targets = batch_targets.reshape(-1, 1)\n",
    "\n",
    "            loss= custom_train_step(model, batch_inputs, batch_targets)\n",
    "            epoch_loss += tf.reduce_mean(loss)\n",
    "\n",
    "        avg_loss = epoch_loss / num_batches\n",
    "        print(\"Epoch {}: Loss: {:.4f}\".format(epoch + 1, avg_loss.numpy()))\n",
    "\n",
    "        # 在每个epoch结束后进行验证\n",
    "        # Reshape y_test to (batch_size, 1) to match the model output shape\n",
    "        y_test_reshaped = y_test.reshape(-1, 1)\n",
    "        val_loss = model.evaluate(x_test, y_test_reshaped, verbose=0)\n",
    "        print(\"Validation Loss: {:.4f}\".format(val_loss[0]))\n",
    "        path='innine'+'/'+'weight.'+str(epoch+1)+'.txt'\n",
    "        np.savetxt(path,model.layers[1].get_weights()[0])\n",
    "        path1='innine'+'/'+'weight_re.'+str(epoch+1)+'.txt'\n",
    "        np.savetxt(path1,model.layers[1].get_weights()[1])\n",
    "        path2='innine'+'/'+'loss.'+str(epoch+1)+'.txt'\n",
    "        np.savetxt(path2,[avg_loss.numpy(),val_loss[0]])\n",
    "\n",
    "# 训练模型\n",
    "custom_train_loop(model_s, x_train, y_train, x_test, y_test, epochs=1000, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4a98cc67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-24 14:20:46.067093: I tensorflow/core/common_runtime/executor.cc:1209] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-24 14:20:46.069530: I tensorflow/core/common_runtime/executor.cc:1209] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-24 14:20:46.071403: I tensorflow/core/common_runtime/executor.cc:1209] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow.keras.regularizers\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.datasets import imdb\n",
    "import numpy as np\n",
    "import matplotlib as matplot\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "import math\n",
    "# from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger, EarlyStopping\n",
    "import pickle\n",
    "from tensorflow.keras.layers import GaussianNoise\n",
    "path='LSTM/'+'weights.112'+'.hdf5'\n",
    "model=load_model(path)\n",
    "weights = model.layers[0].get_weights()\n",
    "weights3= model.layers[1].get_weights()[0]\n",
    "weights2= model.layers[2].get_weights()\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import math\n",
    "import numpy as np\n",
    "import math\n",
    "import numpy\n",
    "import random\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tensorflow.keras import models, Model\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "import matplotlib as mpl\n",
    "from matplotlib import cm\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from matplotlib.colors import ListedColormap\n",
    "import multiprocessing\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models, Model\n",
    "import numpy as np\n",
    "\n",
    "class DataSet():\n",
    "    def __init__(self, num_words=None, seed=113, maxlen=100, train_portion=0.5, long_first=False):\n",
    "        \"\"\"\n",
    "        :param num_words: Size of vocabulary, all words out-of-vocabulary will be marked as [unk]\n",
    "        :param seed: Seed for randomly shuffling dataset\n",
    "        :param maxlen: Maximum length of review, reviews longer than maxlen will be truncated, reviews shorter than\n",
    "        maxlen will be padded to maxlen\n",
    "        :param train_portion: Portion of dataset to allocate to training data, test data will be 1-train_portion\n",
    "        :param long_first: Sort reviews by length\n",
    "        \"\"\"\n",
    "        # retrieve IMDb data, x is a sequence containing movie review,\n",
    "        # y is a label indicating if it is positive or negative sentiment\n",
    "        (self.x_train, self.y_train), (self.x_test, self.y_test) = imdb.load_data(num_words=num_words, seed=seed)\n",
    "\n",
    "        if long_first:\n",
    "            self.y_train = [y for _, y in sorted(zip(self.x_train, self.y_train), key=lambda s: len(s[0]), reverse=True)]\n",
    "            self.x_train = sorted(self.x_train, key=len, reverse=True)\n",
    "\n",
    "            self.y_test = [y for _, y in sorted(zip(self.x_test, self.y_test), key=lambda s: len(s[0]), reverse=True)]\n",
    "            self.x_test = sorted(self.x_test, key=len, reverse=True)\n",
    "\n",
    "        self.total_length = [len(x) for x in self.x_train]\n",
    "        temp = [len(x) for x in self.x_test]\n",
    "        self.total_length.extend(temp)\n",
    "\n",
    "        # padding sequences to all be of the same length\n",
    "        self.x_train = pad_sequences(self.x_train, maxlen=maxlen, padding='post', truncating='post')\n",
    "        self.x_test = pad_sequences(self.x_test, maxlen=maxlen, padding='post', truncating='post')\n",
    "\n",
    "        self.split_data(train_portion)\n",
    "\n",
    "        self.word_index = imdb.get_word_index()\n",
    "        self.word_to_index = {word:id + 3 for word, id in self.word_index.items()}\n",
    "        self.word_to_index[\"[pad]\"] = 0\n",
    "        self.word_to_index[\"[start]\"] = 1\n",
    "        self.word_to_index[\"[unk]\"] = 2\n",
    "        self.word_to_index[\"[unused]\"] = 3\n",
    "        self.index_to_word = {i:word for (word, i) in self.word_to_index.items()}\n",
    "        self.form_vocab()\n",
    "\n",
    "    def form_vocab(self):\n",
    "        i2w_vocab = {}\n",
    "        w2i_vocab = {}\n",
    "\n",
    "        for sentence in self.x_train:\n",
    "            for id in sentence:\n",
    "                if id not in i2w_vocab:\n",
    "                    word = self.index_to_word[id]\n",
    "                    i2w_vocab[id] = word\n",
    "                    w2i_vocab[word] = id\n",
    "\n",
    "        for sentence in self.x_test:\n",
    "            for id in sentence:\n",
    "                if id not in i2w_vocab:\n",
    "                    word = self.index_to_word[id]\n",
    "                    i2w_vocab[id] = word\n",
    "                    w2i_vocab[word] = id\n",
    "        self.i2w_vocab = i2w_vocab\n",
    "        self.w2i_vocab = w2i_vocab\n",
    "\n",
    "    def get_data(self):\n",
    "        return (self.x_train, self.y_train), (self.x_test, self.y_test), (self.train_length, self.test_length)\n",
    "\n",
    "    def get_vocab_length(self):\n",
    "        return len(self.i2w_vocab)\n",
    "\n",
    "    # splits data into ratio train:test -> (train_portion:1-train_portion)\n",
    "    def split_data(self, train_portion):\n",
    "        x = np.concatenate((self.x_train, self.x_test), axis=0)\n",
    "        y = np.concatenate((self.y_train, self.y_test), axis=0)\n",
    "        self.train_length, self.test_length = self.total_length[:math.floor(train_portion * len(x))], \\\n",
    "                                              self.total_length[math.floor(train_portion * len(x)):]\n",
    "        self.x_train, self.x_test = x[:math.floor(train_portion * len(x))], \\\n",
    "                                    x[math.floor(train_portion * len(x)):]\n",
    "        self.y_train, self.y_test = y[:math.floor(train_portion * len(y))], \\\n",
    "                                    y[math.floor(train_portion * len(y)):]\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dce842d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
